

# WI2 Workshop:  
### Wearable designing for attention and executive control in implicit interaction 
#### Workshop at MUM ‘20- Essen, Germany - November 22-25th 2020

![logo](/images/wi2s.png)

Traditional computational tools aim to improve productivity and enhance attention through notifications and time management schemes; however, the user's attention and other cognitive resources are limited. Consequently, the usage of these tools inherently recruits valuable user's resources. On the other hand, wearables physiological computing enables researchers to design new implicit interactions through physiological inputs/outputs without requiring and redirecting attentional resources. (not taking the attention away from the primary task). This scenario keeps opening a broad range of applications for persuasive wearable computing.

In this workshop, we will explore/discuss how wearable computers/sensors/actuators can help optimize human cognitive (i. e., attention, memory, executive functions) and physical resources in daily life situations and assist people with impairments.

We will follow the premise that "We have to optimize the available resources instead of pushing them over the healthy boundaries." Having that in mind, we have three main goals:

- Understand how human attention and executive control works
- Ways to deliver non-disruptive signals to the users.
- How to develop Persuasive applications using wearable devices


## Workshop Topic and Goals


Given their current relevance for ubiquitous computing to enhance seamless human-computer integration, the topics of interes include, but are not limited to:

- Mobile Body/Brain Imaging
- Wearable Computing
- Wearable Haptics
- Peripheral Interaction
- Implicit Interaction
- Sensory Augmentation
- Sensory Substitution
- Brain Computer Interfaces
- Accountability of attention
- Task Interruption and Resumption
- Task Engagement in AR applications
- Attention orienting in AR
- Physiological Correlates of Distraction
- Multisensory Integration for the information-delivery
- Task Switching 



## Call for participation

This half-day workshop provides a multi-disciplinary forum for researchers and practitioners working on Wearable Computing or/and Implicit interaction. Participants are asked to submit an expression of interest and describe their recent or future work in the Workshop’s areas of interest.
Submissions should be done through the following [form](https://docs.google.com/forms/d/1FgwfFdxbEVgBfsu7ruZoEpHAVvVbqeXxq2djmXE5VgM/prefill)




## Important Information

::: warning <Badge text="Format" vertical="middle"/> 
Duration: Half-day (4 Hours)  
Place: Online
:::
::: warning <Badge text="Deadlines" vertical="middle"/> 
- Submission for expression of interests: **Friday 6th November 2020**   
- Decision to authors: Tuesday, 13th November, 2020   
- MUM Conference: 22-25 November, 2020   
:::

## Schedule

To be defined

## What we gonna learn?

 > In this workshop, we will deepen our knowledge about how attentional and executive processes can help us design wearable devices to support users in situations that require human intervention.

 > Remembering to execute intentions, resuming tasks from interruptions, sustaining focused performance are examples of daily activities which performance is driven by attentional processes.

## Organizers & Contact

In case you have questions regarding the workshop, feel free to contact the organizers.


<figure class="snip1336">
  <img src="https://ubicomp.net/wp/wp-content/uploads/2017/11/lmu.jpg" alt="sample87" />
  <figcaption>
    <img src="https://pbs.twimg.com/profile_images/1008496811538042880/Fz73izKE_400x400.jpg" alt="profile-sample4" class="profile" />
    <h2>Steeven Villa<span>PhD Researcher</span><span>Human Centered ubiquitous Media Research Group</span> <span>LMU Munich</span></h2>
    <p>Computer Scientist and Mechatronic Engineer, and PhD Student in the Human-Centered Ubiquitous Media group at LMU Munich. My research field is human computer interaction, particularly human augmentation/Human-computer Integration/Human-computer symbiosis. I'm highly interested in finding new ways to interact with the external environment through embodied/wearable devices. </p>
    <a href="https://twitter.com/steevenvs" class="follow">Follow</a>
    <a href="/" class="info">More Info</a>
  </figcaption>
</figure>


<figure class="snip1336">
  <img src="https://ubicomp.net/wp/wp-content/uploads/2017/11/lmu.jpg" alt="sample87" />
  <figcaption>
    <img src="https://media-exp1.licdn.com/dms/image/C5603AQFftqOfVN3DuA/profile-displayphoto-shrink_800_800/0?e=1608163200&v=beta&t=Q5OI2vdgmvVZupxkCVsOu_zDzpKRBUMTgXrSqBmgMYY" alt="profile-sample4" class="profile" />
    <h2>Francesco Chiossi<span>PhD Researcher</span><span>Human Centered ubiquitous Media Research Group</span> <span>LMU Munich</span></h2>
    <p>I  focus on the relation between human factors and cognitive neuroscience. The first area concerns human performance in human-machine systems, particularly the role of human attention, memory, and perception in designing products and adaptive systems. The second area of research is cognitive neuroscience, investigated with cognitive electrophysiology instruments (EEG and MEG).</p>
    <a href="https://twitter.com/francescochios3" class="follow">Follow</a>
    <a href="https://www.um.informatik.uni-muenchen.de/personen/mitarbeiter/chiossi/index.html" class="info">More Info</a>
  </figcaption>
</figure>

<style>

@import url(https://fonts.googleapis.com/css?family=Roboto:300,400,600);
.snip1336 {
  font-family: 'Roboto', Arial, sans-serif;
  position: relative;
  overflow: hidden;
  margin: 10px;
  min-width: 230px;
  max-width: 315px;
  width: 100%;
  color: #000;
  text-align: left;
  line-height: 1.4em;
  background-color: #ffff;
}
figure {

    float: left;
    margin: 0;
    text-align: center;
    padding: 0;
}
.snip1336 * {
  -webkit-box-sizing: border-box;
  box-sizing: border-box;
  -webkit-transition: all 0.25s ease;
  transition: all 0.25s ease;
}
.snip1336 img {
  max-width: 100%;
  vertical-align: top;
  opacity: 0.85;
}
.snip1336 figcaption {
  width: 100%;
  background-color: #ffffff;
  padding: 25px;
  position: relative;
}
.snip1336 figcaption:before {
  position: absolute;
  content: '';
  bottom: 100%;
  left: 0;
  width: 0;
  height: 0;
  border-style: solid;
  border-width: 010px 0px 00px 300px;
  border-color: transparent transparent transparent #ffffff;
}
.snip1336 figcaption a {
  padding: 5px;
  border: 1px solid #000;
  color: #000;
  font-size: 0.7em;
  text-transform: uppercase;
  margin: 10px 0;
  display: inline-block;
  opacity: 0.65;
  width: 47%;
  text-align: center;
  text-decoration: none;
  font-weight: 600;
  letter-spacing: 1px;
}
.snip1336 figcaption a:hover {
  opacity: 1;
}
.snip1336 .profile {
  border-radius: 50%;
  position: absolute;
  bottom: 100%;
  left: 25px;
  z-index: 1;
  max-width: 90px;
  opacity: 1;
  box-shadow: 0 0 15px rgba(0, 0, 0, 0.3);
}
.profile {
  width:35%
}
.snip1336 .follow {
  margin-right: 4%;
  border-color: #3eaf7c;
  color: #3eaf7c;
}
.snip1336 h2 {
  margin: 0 0 5px;
  font-weight: 300;
}
.snip1336 h2 span {
  display: block;
  font-size: 0.5em;
  color: #3eaf7c;
}
.snip1336 p {
  margin: 0 0 10px;
  font-size: 0.8em;
  letter-spacing: 1px;
  opacity: 0.8;
}

</style>



